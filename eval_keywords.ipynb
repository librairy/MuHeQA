{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2996a0d8",
   "metadata": {},
   "source": [
    "# Experiment Setup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc611c5",
   "metadata": {},
   "source": [
    "## Install dependencies and prepare modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e4d1454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (22.2.2)\n",
      "Requirement already satisfied: flair in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (0.11.1)\n",
      "Requirement already satisfied: janome in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.4.2)\n",
      "Requirement already satisfied: ftfy in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (6.1.1)\n",
      "Requirement already satisfied: mpld3==0.3 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.3)\n",
      "Requirement already satisfied: sentencepiece==0.1.95 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.1.95)\n",
      "Requirement already satisfied: gdown==3.12.2 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (3.12.2)\n",
      "Requirement already satisfied: tabulate in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.8.9)\n",
      "Requirement already satisfied: wikipedia-api in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.5.4)\n",
      "Requirement already satisfied: bpemb>=0.3.2 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.3.3)\n",
      "Requirement already satisfied: regex in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (2022.1.18)\n",
      "Requirement already satisfied: conllu>=4.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (4.4.1)\n",
      "Requirement already satisfied: sqlitedict>=1.6.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (2.0.0)\n",
      "Requirement already satisfied: pptree in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (3.1)\n",
      "Requirement already satisfied: segtok>=1.5.7 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (1.5.11)\n",
      "Requirement already satisfied: huggingface-hub in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.4.0)\n",
      "Requirement already satisfied: gensim>=3.4.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (4.1.2)\n",
      "Requirement already satisfied: deprecated>=1.2.4 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (1.2.13)\n",
      "Requirement already satisfied: torch!=1.8,>=1.5.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (1.10.2)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (2.8.2)\n",
      "Requirement already satisfied: matplotlib>=2.2.3 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (3.5.0)\n",
      "Requirement already satisfied: transformers>=4.0.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (4.16.1)\n",
      "Requirement already satisfied: hyperopt>=0.2.7 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (0.2.7)\n",
      "Requirement already satisfied: lxml in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (4.8.0)\n",
      "Requirement already satisfied: konoha<5.0.0,>=4.0.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (4.6.5)\n",
      "Requirement already satisfied: tqdm>=4.26.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (4.62.3)\n",
      "Requirement already satisfied: langdetect in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (1.0.9)\n",
      "Requirement already satisfied: more-itertools~=8.8.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (8.8.0)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from flair) (1.0.1)\n",
      "Requirement already satisfied: six in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from gdown==3.12.2->flair) (1.16.0)\n",
      "Requirement already satisfied: requests[socks] in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from gdown==3.12.2->flair) (2.26.0)\n",
      "Requirement already satisfied: filelock in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from gdown==3.12.2->flair) (3.4.2)\n",
      "Requirement already satisfied: numpy in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from bpemb>=0.3.2->flair) (1.22.3)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from deprecated>=1.2.4->flair) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from gensim>=3.4.0->flair) (1.7.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from gensim>=3.4.0->flair) (5.2.1)\n",
      "Requirement already satisfied: cloudpickle in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (2.0.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (2.8)\n",
      "Requirement already satisfied: future in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (0.18.2)\n",
      "Requirement already satisfied: py4j in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from hyperopt>=0.2.7->flair) (0.10.9.5)\n",
      "Requirement already satisfied: importlib-metadata<4.0.0,>=3.7.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.10.1)\n",
      "Requirement already satisfied: overrides<4.0.0,>=3.0.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from konoha<5.0.0,>=4.0.0->flair) (3.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (3.0.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (0.11.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (1.3.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from matplotlib>=2.2.3->flair) (4.28.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->flair) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from scikit-learn>=0.21.3->flair) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from torch!=1.8,>=1.5.0->flair) (4.0.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from transformers>=4.0.0->flair) (6.0)\n",
      "Requirement already satisfied: sacremoses in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from transformers>=4.0.0->flair) (0.0.47)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,>=0.10.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from transformers>=4.0.0->flair) (0.11.4)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from ftfy->flair) (0.2.5)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from importlib-metadata<4.0.0,>=3.7.0->konoha<5.0.0,>=4.0.0->flair) (3.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from requests[socks]->gdown==3.12.2->flair) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from requests[socks]->gdown==3.12.2->flair) (2.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from requests[socks]->gdown==3.12.2->flair) (2021.10.8)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from requests[socks]->gdown==3.12.2->flair) (1.7.1)\n",
      "Requirement already satisfied: click in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from sacremoses->transformers>=4.0.0->flair) (7.1.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: coloredlogs in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (15.0.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from coloredlogs) (10.0)\n",
      "Requirement already satisfied: unidecode in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (1.3.4)\n",
      "Requirement already satisfied: fcache in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (0.4.7)\n",
      "Requirement already satisfied: appdirs in /Users/cbadenes/miniforge3/lib/python3.9/site-packages (from fcache) (1.4.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[38;20m2022-08-29 17:04:12,745 - muheqa - DEBUG - initializing Entity class instance... (entity.py:11)\u001b[0m\n",
      "\u001b[38;20m2022-08-29 17:04:18,780 - muheqa - DEBUG - initializing Concept class instance... (concept.py:12)\u001b[0m\n",
      "\u001b[38;20m2022-08-29 17:04:19,462 - muheqa - DEBUG - initializing Keyword class instance... (keyword.py:11)\u001b[0m\n",
      "\u001b[38;20m2022-08-29 17:04:19,463 - muheqa - DEBUG - initializing Entity class instance... (entity.py:11)\u001b[0m\n",
      "\u001b[38;20m2022-08-29 17:04:25,125 - muheqa - DEBUG - initializing Concept class instance... (concept.py:12)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install flair\n",
    "!pip install coloredlogs\n",
    "!pip install unidecode\n",
    "!pip install fcache\n",
    "\n",
    "import coloredlogs, logging\n",
    "import application.logformatter as lf\n",
    "from fcache.cache import FileCache\n",
    "\n",
    "log_level = logging.DEBUG\n",
    "\n",
    "fh = logging.StreamHandler()\n",
    "fh.setFormatter(lf.CustomFormatter())\n",
    "fh.setLevel(log_level)\n",
    "\n",
    "logger = logging.getLogger('muheqa')\n",
    "logger.addHandler(fh)\n",
    "logger.setLevel(log_level)\n",
    "\n",
    "import application.summary.entity as ent\n",
    "import application.summary.concept as cp\n",
    "import application.summary.keyword as key\n",
    "import application.cache as cache\n",
    "\n",
    "entity_discovery  = ent.Entity()\n",
    "concept_discovery = cp.Concept()\n",
    "keyword_discovery = key.Keyword()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28e8843",
   "metadata": {},
   "source": [
    "## Define Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fbe1d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics are ready\n"
     ]
    }
   ],
   "source": [
    "import unidecode\n",
    "\n",
    "def normalize(label):\n",
    "  return unidecode.unidecode(label.strip()).lower().replace(\" \",\"_\")\n",
    "\n",
    "def precision(tp,fp):\n",
    "  if (fp+tp == 0):\n",
    "    return 0.0\n",
    "  return tp / (fp + tp)\n",
    "\n",
    "def recall(tp,fn):\n",
    "  if (fn+tp == 0):\n",
    "    return 0.0\n",
    "  return tp / (fn + tp)\n",
    "\n",
    "def f1(tp,fp,fn):\n",
    "  p = precision(tp,fp)\n",
    "  r = recall(tp,fn)\n",
    "  if (p+r == 0):\n",
    "    return 0.0\n",
    "  return 2 * ((p*r)/(p+r))\n",
    "\n",
    "def average(values):\n",
    "  return sum(values) / len(values) \n",
    "\n",
    "# lists of entity lists\n",
    "def evaluate_labels(true_list,pred_list):\n",
    "  tp, tn, fp, fn = 0, 0, 0, 0\n",
    "  precision_list, recall_list, f1_list = [], [], []\n",
    "  empty_values = 0\n",
    "  for index in range(len(true_list)):\n",
    "    # normalize entities\n",
    "    valid_entities = [normalize(e) for e in true_list[index] if e != '']\n",
    "    predicted_entities = [normalize(e) for e in pred_list[index]]\n",
    "    ptp, ptn, pfp, pfn = 0, 0, 0, 0\n",
    "    if (len(valid_entities)==0):\n",
    "      empty_values += 1\n",
    "    for entity in valid_entities:\n",
    "      if (entity not in predicted_entities):\n",
    "        pfn += 1\n",
    "    for entity in predicted_entities:\n",
    "      if (entity in valid_entities):\n",
    "        ptp += 1\n",
    "      else:\n",
    "        pfp += 1    \n",
    "    precision_list.append(precision(ptp,pfp))\n",
    "    recall_list.append(recall(ptp,pfn))\n",
    "    f1_list.append(f1(ptp,pfp,pfn))\n",
    "    tp += ptp\n",
    "    tn += ptn\n",
    "    fp += pfp\n",
    "    fn += pfn  \n",
    "  return  {\n",
    "      'total': index,\n",
    "      'empty': empty_values,\n",
    "      'tp': tp,\n",
    "      'tn': tn, \n",
    "      'fp': fp,\n",
    "      'fn':fn,\n",
    "      'micro-precision': precision(tp,fp),\n",
    "      'micro-recall': recall(tp,fn),\n",
    "      'micro-f1': f1(tp,fp,fn),\n",
    "      'macro-precision': average(precision_list),\n",
    "      'macro-recall': average(recall_list),\n",
    "      'macro-f1': average(f1_list)\n",
    "  }\n",
    "\n",
    "print(\"metrics are ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a283235",
   "metadata": {},
   "source": [
    "## Load SOTA Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e0adc0",
   "metadata": {},
   "source": [
    "### FLERT\n",
    "\n",
    "From paper: Schweter, Stefan and A. Akbik. “FLERT: Document-Level Features for Named Entity Recognition.” ArXiv abs/2011.06993 (2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d02ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Sentence\n",
    "from flair.models import SequenceTagger\n",
    "\n",
    "flert_tagger = SequenceTagger.load(\"flair/ner-english-large\")\n",
    "flert_cache = cache.Cache(\"muheqa_flert\")\n",
    "\n",
    "def get_entities_by_flert(text):\n",
    "    if (flert_cache.exists(text)):\n",
    "        return flert_cache.get(text)\n",
    "    sentence = Sentence(text)\n",
    "    flert_tagger.predict(sentence)\n",
    "    entities = []\n",
    "    for entity in sentence.get_spans('ner'):\n",
    "        entities.append(entity.text)\n",
    "    flert_cache.set(text,entities)\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddd9bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_entities_by_flert(\"George Washington went to Washington\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60fa521",
   "metadata": {},
   "source": [
    "### BERT-based NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc8890d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_model = AutoModelForTokenClassification.from_pretrained(\"dslim/bert-base-NER\")\n",
    "bert_cache = cache.Cache(\"muheqa_bert\")\n",
    "\n",
    "\n",
    "bert_nlp = pipeline(\"ner\", model=bert_model, tokenizer=bert_tokenizer)\n",
    "\n",
    "def get_entities_by_bert_base(text):\n",
    "    if (bert_cache.exists(text)):\n",
    "        return bert_cache.get(text)\n",
    "    entities = []\n",
    "    entity = \"\"\n",
    "    index = -1\n",
    "    offset = -1\n",
    "    for token in bert_nlp(text):\n",
    "        if (index == -1):\n",
    "            index = token['index']\n",
    "            offset = token['start']\n",
    "        word = token['word']\n",
    "        if (word[0] == '#'):\n",
    "            word = token['word'].replace(\"#\",\"\")        \n",
    "        if (token['start']== offset):\n",
    "            entity += word\n",
    "        elif (token['index']-index < 2):\n",
    "            entity += \" \" + word\n",
    "        else:\n",
    "            entities.append(entity)\n",
    "            entity = word\n",
    "        index = token['index']\n",
    "        offset = token['end']\n",
    "    if (len(entity) > 0):    \n",
    "        entities.append(entity)\n",
    "    bert_cache.set(text,entities)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f15068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "get_entities_by_bert_base(\"George Washington went to Washington\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd40da21",
   "metadata": {},
   "source": [
    "### RoBERTA-based NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9efce6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "roberta_tokenizer = AutoTokenizer.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "roberta_model = AutoModelForTokenClassification.from_pretrained(\"Jean-Baptiste/roberta-large-ner-english\")\n",
    "roberta_cache = cache.Cache(\"muheqa_roberta\")\n",
    "\n",
    "roberta_nlp = pipeline(\"ner\", model=roberta_model, tokenizer=roberta_tokenizer)\n",
    "\n",
    "def get_entities_by_roberta_base(text):\n",
    "    if (roberta_cache.exists(text)):\n",
    "        return roberta_cache.get(text)\n",
    "    entities = []\n",
    "    entity = \"\"\n",
    "    index = -1\n",
    "    offset = -1\n",
    "    for token in roberta_nlp(text):\n",
    "        if (index == -1):\n",
    "            index = token['index']\n",
    "            offset = token['start']\n",
    "        word = token['word']\n",
    "        if (word[0] == '#'):\n",
    "            word = token['word'].replace(\"#\",\"\")        \n",
    "        if (token['start']== offset):\n",
    "            entity += word\n",
    "        elif (token['index']-index < 2):\n",
    "            entity += \" \" + word\n",
    "        else:\n",
    "            entities.append(entity.replace(\"Ġ\",\"\"))\n",
    "            entity = word\n",
    "        index = token['index']\n",
    "        offset = token['end']\n",
    "    if (len(entity) > 0):    \n",
    "        entities.append(entity.replace(\"Ġ\",\"\"))\n",
    "    roberta_cache.set(text,entities)\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31409fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unit test\n",
    "get_entities_by_roberta_base(\"George Washington went to Washington\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d83bc43",
   "metadata": {},
   "source": [
    "# Evaluation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46808452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def json_file(name):\n",
    "  return name+\"-keywords.json\"\n",
    "\n",
    "def csv_file(name):\n",
    "  return name+\"-keywords.csv\"\n",
    "\n",
    "def evaluate_data(name,dataframe):\n",
    "  l1, l2, l3, l4 = [], [], [], []\n",
    "  total = 0\n",
    "  for index, row in dataframe.iterrows():\n",
    "      question = row['question']\n",
    "      print(index,\":\",question)\n",
    "      l1.append(keyword_discovery.get(question))\n",
    "      l2.append(get_entities_by_flert(question))\n",
    "      l3.append(get_entities_by_bert_base(question))\n",
    "      l4.append(get_entities_by_roberta_base(question))\n",
    "      total += 1\n",
    "  dataframe['MuHeQA_Keywords']=l1\n",
    "  dataframe['FLERT_NER']=l2\n",
    "  dataframe['BERT_NER']=l3\n",
    "  dataframe['RoBERTA_NER']=l4\n",
    "  clear_output(wait=True)\n",
    "  print(total,\"questions analyzed!\")\n",
    "  dataframe.to_json(json_file(name), orient='split')\n",
    "  dataframe.to_csv(csv_file(name))\n",
    "  return dataframe\n",
    "\n",
    "def make_report(name,additional=[]):\n",
    "  \n",
    "  df = pd.read_json(json_file(name), orient='split')\n",
    "  y_true =df['entities'].tolist()\n",
    "  results = []\n",
    "  for col in df.columns:\n",
    "    if (col == 'question') or (col == 'entities'):\n",
    "      continue\n",
    "    y_pred = df[col].tolist()\n",
    "    result = evaluate_labels(y_true,y_pred)\n",
    "    result['model']=col\n",
    "    results.append(result)\n",
    "\n",
    "  for row in additional:\n",
    "    results.append(row)\n",
    "\n",
    "  df_results = pd.DataFrame(results)\n",
    "  return df_results\n",
    "\n",
    "print(\"evaluation methods are ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4ffe62",
   "metadata": {},
   "source": [
    "# Basic Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aea0617",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#query = \"what does 2674 pandarus orbit?\"\n",
    "query = \"in which country was overnight delivery filmed in?\"\n",
    "logger.info(\"Query: '\" + query + \"'\")\n",
    "logger.info(\"Entities:\"+ str(entity_discovery.get(query)))\n",
    "logger.info(\"Concepts:\"+ str(concept_discovery.get(query)))\n",
    "logger.info(\"Keywords:\"+ str(keyword_discovery.get(query)))\n",
    "logger.info(\"FLERT:\"+ str(get_entities_by_flert(query)))\n",
    "logger.info(\"BERT:\"+ str(get_entities_by_bert_base(query)))\n",
    "logger.info(\"RoBERTA:\"+ str(get_entities_by_roberta_base(query)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79001455",
   "metadata": {},
   "source": [
    "# SimpleQuestions Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01beb909",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('datasets/simple_questions/wsq-labels.csv', index_col=0)\n",
    "# inverse predicates contain no entity. In all other cases, the entity corresponds to the subject.\n",
    "entities = []\n",
    "questions = []\n",
    "for index, row in df.iterrows():\n",
    "    entity = row['subject_label']  \n",
    "    question = row['question']\n",
    "    if (entity.lower() in question.lower()):\n",
    "        entities.append([entity])\n",
    "        questions.append(question)\n",
    "sq_df = pd.DataFrame(list(zip(questions, entities)),columns =['question', 'entities'])\n",
    "sq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64493b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25df4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_data('sq_results',sq_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffef58",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_report('sq_results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d3245e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
